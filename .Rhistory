model <- polr(foreclosure_quantile ~ avg_bed + mortgaged_2010 + pct_1_bd, data = train_data, Hess = TRUE)
# Summarize the model
summary(model)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model <- polr(foreclosure_quantile ~ avg_bed + mortgaged_2010 + pct_1_bd, data + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015 = train_data, Hess = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model <- polr(foreclosure_quantile ~ avg_bed + mortgaged_2010 + pct_1_bd, data + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015, data = train_data, Hess = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model <- polr(foreclosure_quantile ~ avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015, data = train_data, Hess = TRUE)
# Summarize the model
summary(model)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model <- polr(foreclosure_quantile ~ foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2015_2020 + avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015, data = train_data, Hess = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model <- polr(foreclosure_quantile ~ foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2015_2020 + avg_bed + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015, data = train_data, Hess = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2015_2020 + avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015, data = train_data, Hess = TRUE)
# Summarize the model
summary(model)
knitr::opts_chunk$set(echo = TRUE)
# Predict on test data
predictions <- predict(model, newdata = test_data, type = "response")
knitr::opts_chunk$set(echo = TRUE)
# Predict on test data
predictions <- predict(model_5, newdata = test_data, type = "class")
# Use the training dataset for model fitting
model_5 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2015_2020 + avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_5)
# Predict on test data
predictions <- predict(model_5, newdata = test_data, type = "class")
# View the predictions
print(predictions)
View(test_data)
knitr::opts_chunk$set(echo = TRUE)
# Extract the actual values from test_data
actual_values <- test_data$foreclosure_quantile
# Compare predicted values with actual values
comparison <- data.frame(Actual = actual_values, Predicted = predictions)
# Count the number of matching rows
matching_rows <- sum(comparison$Actual == comparison$Predicted)
# Calculate the total number of rows
total_rows <- nrow(comparison)
# Calculate the percentage of matching rows
matching_percentage <- (matching_rows / total_rows) * 100
# Print the percentage
cat("Percentage of matching rows:", matching_percentage, "%\n")
knitr::opts_chunk$set(echo = TRUE)
# Create a new column "foreclosure_quantile" based on quantiles of foreclosure_pc_2020
pg_foreclosures_per_tract_log_reg <- pg_foreclosures_per_tract %>%
mutate(foreclosure_quantile = ntile(foreclosure_pc_2020, 5),
high_or_low_foreclosure = case_when(
foreclosure_quantile %in% c(1, 2, 3) ~ 0,
foreclosure_quantile %in% c(4, 5) ~ 1,
))
knitr::opts_chunk$set(echo = TRUE)
set.seed(123) # For reproducibility
test_percent <- 0.2
indices <- sample(1:nrow(pg_foreclosures_per_tract_log_reg),
size = round(test_percent * nrow(pg_foreclosures_per_tract_log_reg)))
train_data_2 <- pg_foreclosures_per_tract_log_reg[-indices, ]
test_data_2 <- pg_foreclosures_per_tract_log_reg[indices, ]
knitr::opts_chunk$set(echo = TRUE)
#Train Naive Bayes Model
nb_model <- naiveBayes(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2015_2020 + avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015, data = train_data_2)
View(nb_model)
knitr::opts_chunk$set(echo = TRUE)
# Define coefficients
coefficients <- coef(model_5)
# Calculate log-odds (which are just the coefficients)
log_odds <- coefficients
# Calculate odds ratios
odds_ratios <- exp(coefficients)
# Display the results
result <- data.frame(
Predictor_Variable = names(coefficients),
Log_Odds = log_odds,
Odds_Ratio = odds_ratios
)
print(result)
knitr::opts_chunk$set(echo = TRUE)
foreclosures_per_tract_naive_bayes <- foreclosures_per_tract_log_reg %>%
mutate(
nhwhite_2020_quantile = cut(nhwhite_2020, breaks = quantile(nhwhite_2020, probs = seq(0, 1, by = 0.2)), labels = FALSE),
foreclosure_pc_2010_quantile = cut(foreclosure_pc_2010, breaks = quantile(foreclosure_pc_2010, probs = seq(0, 1, by = 0.2)), labels = FALSE),
tract_medage_2020_quantile = cut(tract_medage_2020, breaks = quantile(tract_medage_2020, probs = seq(0, 1, by = 0.2)), labels = FALSE),
poverty_2010_quantile = cut(poverty_2010, breaks = quantile(poverty_2010, probs = seq(0, 1, by = 0.2)), labels = FALSE),
medincome_change_2015_2020_quantile = cut(medincome_change_2015_2020, breaks = quantile(medincome_change_2015_2020, probs = seq(0, 1, by = 0.2)), labels = FALSE),
avg_bed_quantile = cut(avg_bed, breaks = quantile(avg_bed, probs = seq(0, 1, by = 0.2)), labels = FALSE),
mortgaged_2010_quantile = cut(mortgaged_2010, breaks = quantile(mortgaged_2010, probs = seq(0, 1, by = 0.2)), labels = FALSE),
pct_1_bd_quantile = cut(pct_1_bd, breaks = quantile(pct_1_bd, probs = seq(0, 1, by = 0.2)), labels = FALSE),
pct_built_pre_1960_quantile = cut(pct_built_pre_1960, breaks = quantile(pct_built_pre_1960, probs = seq(0, 1, by = 0.2)), labels = FALSE),
pct_built_2000_2009_quantile = cut(pct_built_2000_2009, breaks = quantile(pct_built_2000_2009, probs = seq(0, 1, by = 0.2)), labels = FALSE),
mortgage_change_2010_2015_quantile = cut(mortgage_change_2010_2015, breaks = quantile(mortgage_change_2010_2015, probs = seq(0, 1, by = 0.2)), labels = FALSE)
)
knitr::opts_chunk$set(echo = TRUE)
pg_foreclosures_per_tract_naive_bayes <- pg_foreclosures_per_tract_log_reg %>%
mutate(
nhwhite_2020_quantile = cut(nhwhite_2020, breaks = quantile(nhwhite_2020, probs = seq(0, 1, by = 0.2)), labels = FALSE),
foreclosure_pc_2010_quantile = cut(foreclosure_pc_2010, breaks = quantile(foreclosure_pc_2010, probs = seq(0, 1, by = 0.2)), labels = FALSE),
tract_medage_2020_quantile = cut(tract_medage_2020, breaks = quantile(tract_medage_2020, probs = seq(0, 1, by = 0.2)), labels = FALSE),
poverty_2010_quantile = cut(poverty_2010, breaks = quantile(poverty_2010, probs = seq(0, 1, by = 0.2)), labels = FALSE),
medincome_change_2015_2020_quantile = cut(medincome_change_2015_2020, breaks = quantile(medincome_change_2015_2020, probs = seq(0, 1, by = 0.2)), labels = FALSE),
avg_bed_quantile = cut(avg_bed, breaks = quantile(avg_bed, probs = seq(0, 1, by = 0.2)), labels = FALSE),
mortgaged_2010_quantile = cut(mortgaged_2010, breaks = quantile(mortgaged_2010, probs = seq(0, 1, by = 0.2)), labels = FALSE),
pct_1_bd_quantile = cut(pct_1_bd, breaks = quantile(pct_1_bd, probs = seq(0, 1, by = 0.2)), labels = FALSE),
pct_built_pre_1960_quantile = cut(pct_built_pre_1960, breaks = quantile(pct_built_pre_1960, probs = seq(0, 1, by = 0.2)), labels = FALSE),
pct_built_2000_2009_quantile = cut(pct_built_2000_2009, breaks = quantile(pct_built_2000_2009, probs = seq(0, 1, by = 0.2)), labels = FALSE),
mortgage_change_2010_2015_quantile = cut(mortgage_change_2010_2015, breaks = quantile(mortgage_change_2010_2015, probs = seq(0, 1, by = 0.2)), labels = FALSE)
)
knitr::opts_chunk$set(echo = TRUE)
pg_foreclosures_per_tract_naive_bayes <- pg_foreclosures_per_tract_log_reg %>%
mutate(
nhwhite_2020_quantile = ntile(nhwhite_2020, 5),
foreclosure_pc_2010_quantile = ntile(foreclosure_pc_2010, 5),
tract_medage_2020_quantile = ntile(tract_medage_2020, 5),
poverty_2010_quantile = ntile(poverty_2010, 5),
medincome_change_2015_2020_quantile = ntile(medincome_change_2015_2020, 5),
avg_bed_quantile = ntile(avg_bed, 5),
mortgaged_2010_quantile = ntile(mortgaged_2010, 5),
pct_1_bd_quantile = ntile(pct_1_bd, 5),
pct_built_pre_1960_quantile = ntile(pct_built_pre_1960, 5),
pct_built_2000_2009_quantile = ntile(pct_built_2000_2009, 5),
mortgage_change_2010_2015_quantile = ntile(mortgage_change_2010_2015, 5)
)
View(pg_foreclosures_per_tract_naive_bayes)
knitr::opts_chunk$set(echo = TRUE)
set.seed(123) # For reproducibility
test_percent <- 0.2
indices <- sample(1:nrow(pg_foreclosures_per_tract_naive_bayes),
size = round(test_percent * nrow(pg_foreclosures_per_tract_naive_bayes)))
train_data_2 <- pg_foreclosures_per_tract_naive_bayes[-indices, ]
test_data_2 <- pg_foreclosures_per_tract_naive_bayes[indices, ]
knitr::opts_chunk$set(echo = TRUE)
#Train Naive Bayes Model
# Define the formula for the Naive Bayes model
formula <- foreclosure_quantile ~ nhwhite_2020_quantile +
foreclosure_pc_2010_quantile +
tract_medage_2020_quantile +
poverty_2010_quantile +
medincome_change_2015_2020_quantile +
avg_bed_quantile +
mortgaged_2010_quantile +
pct_1_bd_quantile +
pct_built_pre_1960_quantile +
pct_built_2000_2009_quantile +
mortgage_change_2010_2015_quantile
# Fit the Naive Bayes model
naive_bayes_model <- naiveBayes(formula, data = pg_foreclosures_per_tract_naive_bayes)
# Summary of the model
summary(naive_bayes_model)
knitr::opts_chunk$set(echo = TRUE)
# Predict using the Naive Bayes model
predictions <- predict(naive_bayes_model, newdata = pg_foreclosures_per_tract_naive_bayes)
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, pg_foreclosures_per_tract_naive_bayes$foreclosure_quantile)
knitr::opts_chunk$set(echo = TRUE)
# Convert the predictions to factor with the same levels as the actual target variable
predictions <- factor(predictions, levels = levels(pg_foreclosures_per_tract_naive_bayes$foreclosure_quantile))
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, pg_foreclosures_per_tract_naive_bayes$foreclosure_quantile)
knitr::opts_chunk$set(echo = TRUE)
# Check levels in training and testing datasets
train_levels <- levels(pg_foreclosures_per_tract_naive_bayes$foreclosure_quantile)
test_levels <- levels(predictions)
# Check for any differences
if (!identical(train_levels, test_levels)) {
# If levels are different, set the levels to be the same in both datasets
levels(predictions) <- train_levels
}
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, pg_foreclosures_per_tract_naive_bayes$foreclosure_quantile)
knitr::opts_chunk$set(echo = TRUE)
# Check levels in training and testing datasets
train_levels <- levels(pg_foreclosures_per_tract_naive_bayes$foreclosure_quantile)
test_levels <- levels(predictions)
# Check for any differences
if (!identical(train_levels, test_levels)) {
# If levels are different, set the levels to be the same in both datasets
levels(predictions) <- train_levels
}
knitr::opts_chunk$set(echo = TRUE)
# Convert the predictions to factor with the same levels as the actual target variable
predictions <- factor(predictions, levels = levels(pg_foreclosures_per_tract_naive_bayes$foreclosure_quantile))
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, pg_foreclosures_per_tract_naive_bayes$foreclosure_quantile)
knitr::opts_chunk$set(echo = TRUE)
# Predict using the Naive Bayes model
predictions <- predict(naive_bayes_model, newdata = pg_foreclosures_per_tract_naive_bayes)
# Convert the predictions to factor with the same levels as the actual target variable
predictions <- factor(predictions, levels = levels(pg_foreclosures_per_tract_naive_bayes$foreclosure_quantile))
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, pg_foreclosures_per_tract_naive_bayes$foreclosure_quantile)
knitr::opts_chunk$set(echo = TRUE)
# Predict using the Naive Bayes model
predictions <- predict(naive_bayes_model, newdata = pg_foreclosures_per_tract_naive_bayes)
knitr::opts_chunk$set(echo = TRUE)
# Predict using the Naive Bayes model
predictions <- predict(naive_bayes_model, newdata = pg_foreclosures_per_tract_naive_bayes)
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, pg_foreclosures_per_tract_naive_bayes$foreclosure_quantile)
View(pg_foreclosures_per_tract_naive_bayes)
knitr::opts_chunk$set(echo = TRUE)
# Predict using the Naive Bayes model
predictions <- predict(naive_bayes_model, newdata = test_data_2)
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data_2$foreclosure_quantile)
View(test_data_2)
knitr::opts_chunk$set(echo = TRUE)
# Predict using the Naive Bayes model
predictions <- predict(naive_bayes_model, newdata = test_data_2)
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, factor(test_data_2$foreclosure_quantile,levels=1:5))
# Print the confusion matrix
print(confusion_matrix)
knitr::opts_chunk$set(echo = TRUE)
# Predict using the Naive Bayes model
predictions <- predict(naive_bayes_model, newdata = test_data_2)
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, factor(test_data_2$foreclosure_quantile, levels = 1:5))
# Print the confusion matrix
print(confusion_matrix)
knitr::opts_chunk$set(echo = TRUE)
#Train Naive Bayes Model
# Define the formula for the Naive Bayes model
formula <- foreclosure_quantile ~ nhwhite_2020 +
foreclosure_pc_2010 +
tract_medage_2020 +
poverty_2010 +
medincome_change_2015_2020 +
avg_bed +
mortgaged_2010 +
pct_1_bd +
pct_built_pre_1960 +
pct_built_2000_2009 +
mortgage_change_2010_2015
# Fit the Naive Bayes model
naive_bayes_model <- naiveBayes(formula, data = pg_foreclosures_per_tract_naive_bayes)
# Summary of the model
summary(naive_bayes_model)
# Predict using the Naive Bayes model
predictions <- predict(naive_bayes_model, newdata = test_data_2)
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, factor(test_data_2$foreclosure_quantile, levels = 1:5))
# Print the confusion matrix
print(confusion_matrix)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model_5 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2010_2020 + avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_5)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model_5 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2010_2020 + avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015 + tract_homevalue, data = train_data, Hess = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model_5 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2010_2020 + avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015 + tract_homevalue_2020, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_5)
knitr::opts_chunk$set(echo = TRUE)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
pg_foreclosures_per_tract <- read_csv("datasets/pg_foreclosures_per_tract.csv", guess_max = 173)
# Create a new column "foreclosure_quantile" based on quantiles of foreclosure_pc_2020
pg_foreclosures_per_tract_log_reg <- pg_foreclosures_per_tract %>%
mutate(foreclosure_quantile = ntile(foreclosure_pc_2020, 5),
high_or_low_foreclosure = case_when(
foreclosure_quantile %in% c(1, 2, 3) ~ 0,
foreclosure_quantile %in% c(4, 5) ~ 1,
))
# Set the seed for reproducibility
set.seed(123)
# Split the data into 70% training and 30% test sets
train_index <- createDataPartition(pg_foreclosures_per_tract_log_reg$foreclosure_quantile, p = 0.7, list = FALSE)
# Create training and test sets
train_data <- pg_foreclosures_per_tract_log_reg[train_index, ]
test_data <- pg_foreclosures_per_tract_log_reg[-train_index, ]
# Check the dimensions of training and test sets to ensure they have the appropriate number of columns
dim(train_data)
dim(test_data)
# Check data range for numeric variables
numeric_vars <- train_data[sapply(train_data, is.numeric)]
data_range <- sapply(numeric_vars, function(x) c(min = min(x, na.rm = TRUE), max = max(x, na.rm = TRUE)))
print("Data Range:")
print(data_range)
# Check for infinite values in numeric variables
infinite_values <- colSums(sapply(numeric_vars, is.infinite))
print("Infinite Values:")
print(infinite_values)
# Check for collinear variables (using correlation matrix)
correlation_matrix <- cor(numeric_vars)
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.7)
print("Highly Correlated Variables:")
print(colnames(numeric_vars)[highly_correlated])
# Create scatterplot matrix
pairs(train_data[, c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010",
"pct_built_2020_later")])
# Convert foreclosure_quantile to factor to make the thing run (not really sure why I need to specify this, but it's the only thing that worked)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_1)
# Convert foreclosure_quantile to factor
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_2 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2015_2020, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_2)
# Use the training dataset for model fitting
model_3 <- polr(foreclosure_quantile ~ avg_bed + mortgaged_2010 + pct_1_bd, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_3)
# Use the training dataset for model fitting
model_4 <- polr(foreclosure_quantile ~ avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_4)
# Use the training dataset for model fitting
model_5 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2010_2020 + avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015 + tract_homevalue_2020, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_5)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model_5 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2010_2020 + avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_5)
View(pg_foreclosures_per_tract_log_reg)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('gplots')
library(tidyverse)
library(dplyr)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
library(gplots)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
#Read in the geocoded address dataset.
foreclosure_pg_census_tracts <- read_csv("datasets/pg_foreclosure_tract_geocodio.csv", guess_max = 71676) |>
clean_names()
# Calculate the number of unique tracts in the geocoded address dataframe.
tracts_unique <- length(unique(foreclosure_pg_census_tracts$census_tract_code))
print(tracts_unique)
# Perform inner join based on the 'location' column.
pg_foreclosures_with_tracts <- inner_join(foreclosure_pg, foreclosure_pg_census_tracts, by = "location")
# Convert "submitteddate" column to date type.
pg_foreclosures_with_tracts$submitteddate <- as.Date(pg_foreclosures_with_tracts$submitteddate, format = "%m/%d/%Y")
# Extract the year and month from "submitteddate".
pg_foreclosures_with_tracts$year <- year(pg_foreclosures_with_tracts$submitteddate)
pg_foreclosures_with_tracts$month <- month(pg_foreclosures_with_tracts$submitteddate)
# Create a year-month column for eventual plotting.
pg_foreclosures_with_tracts$year_month <- as.Date(paste(pg_foreclosures_with_tracts$year, pg_foreclosures_with_tracts$month, "01", sep = "-"), format = "%Y-%m-%d")
# Print the dataset.
print(pg_foreclosures_with_tracts)
# Order the dataframe by the columns that identify remainins duplicate rows.
pg_foreclosures_with_tracts <- pg_foreclosures_with_tracts[order(pg_foreclosures_with_tracts$propertyid, pg_foreclosures_with_tracts$street_address, pg_foreclosures_with_tracts$city.y, pg_foreclosures_with_tracts$submitteddate), ]
# Keep only the last row from each set of duplicates
pg_foreclosures_filtered <- subset(pg_foreclosures_with_tracts, !duplicated(pg_foreclosures_with_tracts[, c("propertyid", "street_address", "city.y", "submitteddate")], fromLast = TRUE))
# Reset row names if necessary
rownames(pg_foreclosures_filtered) <- NULL
# Filter the pg_foreclosures_filtered dataset to remove rows with missing values in the census_tract_code column.
pg_foreclosures_filtered <- pg_foreclosures_filtered[!is.na(pg_foreclosures_filtered$census_tract_code), ]
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered %>%
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
knitr::opts_chunk$set(echo = TRUE)
wsp_all_stops_2018_2023 <- read_csv("wsp_all_stops_2018_2023")
options(scipen=999)
#install.packages("ggrepel")
#install.packages('ggthemes')
library(tidyverse)
library(lubridate)
library(janitor)
library(ggthemes)
library(tidycensus)
library(ggplot2)
library(ggrepel)
library(tigris)
library(sf)
library(tibble)
library(dplyr)
library(janitor)
library(readr)
library(purrr)
wsp_all_stops_2018_2023 <- read_csv("wsp_all_stops_2018_2023")
# Define the columns to group by
group_columns <- c(
"employee", "race_title", "employee_gender", "badge",
"contact_date", "contact_hour", "highway_type_id",
"road_number", "contact_mile_post", "contact_type"
)
# Find the remaining columns (non-grouping columns)
non_group_columns <- setdiff(colnames(wsp_all_stops_2018_2023), group_columns)
# Define chunk size for processing
chunk_size <- 500000
num_chunks <- ceiling(nrow(wsp_all_stops_2018_2023) / chunk_size)
# File to save results incrementally
output_file <- "possible_multiple_report_single_stop.csv"
# Initialize the output file
if (file.exists(output_file)) {
file.remove(output_file) # Remove existing file to start fresh
}
# Process each chunk
for (i in 1:num_chunks) {
cat("Processing chunk", i, "of", num_chunks, "\n")
# Get the rows for the current chunk
start_row <- (i - 1) * chunk_size + 1
end_row <- min(i * chunk_size, nrow(wsp_all_stops_2018_2023))
chunk <- wsp_all_stops_2018_2023[start_row:end_row, ]
# Group and filter rows within the chunk
chunk_result <- chunk %>%
group_by(across(all_of(group_columns))) %>%
filter(n_distinct(across(all_of(non_group_columns))) > 1) %>%
ungroup()
# Save the chunk result incrementally
if (nrow(chunk_result) > 0) {
write.table(
chunk_result,
output_file,
append = TRUE,
sep = ",",
col.names = !file.exists(output_file), # Add column headers only for the first chunk
row.names = FALSE
)
}
}
cat("Processing complete. Results saved to", output_file, "\n")
possible_multiple_report_single_stop <- read_csv("possible_multiple_report_single_stop.csv")
View(possible_multiple_report_single_stop)
# Filter out rows where 'employee' appears only once
filtered_possible_multiple_report_single_stop <- possible_multiple_report_single_stop %>%
group_by(employee) %>%
filter(n() > 1) %>%
ungroup()
# Filter out rows where 'employee' appears only once
filtered_possible_multiple_report_single_stop <- possible_multiple_report_single_stop %>%
group_by(employee) %>%
filter(n() > 1) %>%
ungroup()
View(wsp_all_stops_2018_2023)
View(filtered_possible_multiple_report_single_stop)
View(filtered_possible_multiple_report_single_stop)
gc()
View(filtered_possible_multiple_report_single_stop)
View(filtered_possible_multiple_report_single_stop)
View(filtered_possible_multiple_report_single_stop)
View(filtered_possible_multiple_report_single_stop)
View(filtered_possible_multiple_report_single_stop)
View(filtered_possible_multiple_report_single_stop)
View(filtered_possible_multiple_report_single_stop)
View(filtered_possible_multiple_report_single_stop)
View(filtered_possible_multiple_report_single_stop)
# Save the filtered dataset
write_csv(filtered_possible_multiple_report_single_stop, "filtered_possible_multiple_report_single_stop.csv")
View(wsp_all_stops_2018_2023)
setwd("C:/Users/paulf/GitHub/hearst_data_test")
View(foreclosure_pg)
knitr::opts_chunk$set(echo = TRUE)
### Load libraries ###
# Core tidyverse
library(tidyverse)
library(lubridate)
library(janitor)
# Visualization
library(ggthemes)
library(ggrepel)
library(leaflet)
# Spatial
library(sf)
library(tigris)
library(tidycensus)
library(sfnetworks)
library(units)
# Graphs and networks
library(igraph)
library(tidygraph)
library(ggraph)
# Databases & I/O
library(DBI)
library(RSQLite)
library(odbc)
library(readxl)
# Performance and parallelism
library(data.table)
library(parallel)
library(furrr)
# Utilities
library(wdman)
